{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e70c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. An activation function in the context of artificial neural networks is a mathematical operation applied to the output of each neuron in a neural network. It introduces non-linearities to the network, allowing it to learn complex patterns and relationships in the data.\n",
    "\n",
    "Q2. Common types of activation functions include:\n",
    "Sigmoid\n",
    "Hyperbolic Tangent (tanh)\n",
    "Rectified Linear Unit (ReLU)\n",
    "Leaky ReLU\n",
    "Softmax\n",
    "\n",
    "Q3. Activation functions affect the training process by introducing non-linearities that enable the network to learn and adapt to complex patterns. They also play a crucial role in addressing issues like the vanishing gradient problem and enabling the network to converge during training.\n",
    "\n",
    "Q4. The sigmoid activation function maps input values to a range between 0 and 1. Its formula is f(x) = 1 / (1 + e^(-x)). Advantages include smooth gradients, making it suitable for gradient-based optimization. Disadvantages include vanishing gradients and the output not centered around zero, which can slow down learning in certain cases.\n",
    "\n",
    "Q5. The Rectified Linear Unit (ReLU) activation function outputs the input directly if it is positive; otherwise, it outputs zero. It's defined as f(x) = max(0, x). Unlike the sigmoid function, ReLU doesn't saturate for positive values, which helps mitigate the vanishing gradient problem.\n",
    "\n",
    "Q6. ReLU has benefits over the sigmoid function, such as avoiding the vanishing gradient problem, being computationally efficient, and promoting sparse activation, making it easier for the network to learn representations.\n",
    "\n",
    "Q7. Leaky ReLU is a variation of ReLU that allows a small, non-zero gradient when the input is negative. It addresses the vanishing gradient problem by providing a path for the gradient to flow during backpropagation, even for negative inputs.\n",
    "\n",
    "Q8. The softmax activation function is used to convert a vector of real numbers into a probability distribution. It's commonly used in the output layer of a neural network for multi-class classification problems, as it normalizes the output values to represent probabilities.\n",
    "\n",
    "Q9. The hyperbolic tangent (tanh) activation function is similar to the sigmoid but maps input values to a range between -1 and 1. It addresses the issue of the output not being centered around zero in the sigmoid function, but it still suffers from the vanishing gradient problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
